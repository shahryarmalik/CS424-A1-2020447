{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSC-f9Kczcu9",
        "outputId": "744ce4dd-f00c-4a9d-ecad-29490a963539"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: IDENTIFIER, Lexeme: /, Line: 1\n",
            "Token: IDENTIFIER, Lexeme: /, Line: 1\n",
            "Token: BOOLEAN_LITERAL, Lexeme: Example, Line: 1\n",
            "Token: BOOLEAN_LITERAL, Lexeme: MiniLang, Line: 1\n",
            "Token: BOOLEAN_LITERAL, Lexeme: program, Line: 1\n",
            "Token: BOOLEAN_LITERAL, Lexeme: a, Line: 2\n",
            "Token: IDENTIFIER, Lexeme: =, Line: 2\n",
            "Token: __main__, Lexeme: 5, Line: 2\n",
            "Token: MINUS, Lexeme: ;, Line: 2\n",
            "Token: BOOLEAN_LITERAL, Lexeme: b, Line: 3\n",
            "Token: IDENTIFIER, Lexeme: =, Line: 3\n",
            "Token: INTEGER_LITERAL, Lexeme: true, Line: 3\n",
            "Token: MINUS, Lexeme: ;, Line: 3\n",
            "Token: BOOLEAN_LITERAL, Lexeme: c, Line: 4\n",
            "Token: IDENTIFIER, Lexeme: =, Line: 4\n",
            "Token: BOOLEAN_LITERAL, Lexeme: a, Line: 4\n",
            "Token: IDENTIFIER, Lexeme: *, Line: 4\n",
            "Token: __main__, Lexeme: 2, Line: 4\n",
            "Token: IDENTIFIER, Lexeme: +, Line: 4\n",
            "Token: __main__, Lexeme: 1, Line: 4\n",
            "Token: MINUS, Lexeme: ;, Line: 4\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "class TokenType:\n",
        "    INTEGER_LITERAL = \"INTEGER_LITERAL\"\n",
        "    BOOLEAN_LITERAL = \"BOOLEAN_LITERAL\"\n",
        "    IDENTIFIER = \"IDENTIFIER\"\n",
        "    PLUS = \"PLUS\"\n",
        "    MINUS = \"MINUS\"\n",
        "    MULTIPLY = \"MULTIPLY\"\n",
        "    DIVIDE = \"DIVIDE\"\n",
        "    ASSIGN = \"ASSIGN\"\n",
        "    EQUAL = \"EQUAL\"\n",
        "    NOTEQUAL = \"NOTEQUAL\"\n",
        "    IF = \"IF\"\n",
        "    ELSE = \"ELSE\"\n",
        "    PRINT = \"PRINT\"\n",
        "    TRUE = \"TRUE\"\n",
        "    FALSE = \"FALSE\"\n",
        "    LPAREN = \"LPAREN\"\n",
        "    RPAREN = \"RPAREN\"\n",
        "    LBRACE = \"LBRACE\"\n",
        "    RBRACE = \"RBRACE\"\n",
        "    SEMICOLON = \"SEMICOLON\"\n",
        "    COMMENT = \"COMMENT\"\n",
        "    ERROR = \"ERROR\"\n",
        "\n",
        "# Define regular expressions for tokens\n",
        "int_literal_pattern = r\"\\d+\"\n",
        "bool_literal_pattern = r\"true|false\"\n",
        "identifier_pattern = r\"[a-zA-Z][a-zA-Z0-9]*\"\n",
        "operators_pattern = r\"[+\\-*/=]=?|!=\"\n",
        "keywords_pattern = r\"if|else|print|true|false\"\n",
        "parentheses_braces_pattern = r\"[(){};]\"\n",
        "comment_pattern = r\"//.*\"\n",
        "\n",
        "# Combine all patterns\n",
        "pattern = f\"({int_literal_pattern})|({bool_literal_pattern})|({identifier_pattern})|({operators_pattern})|({keywords_pattern})|({parentheses_braces_pattern})|({comment_pattern})\"\n",
        "\n",
        "# Tokenize function\n",
        "def tokenize(source_code_file):\n",
        "    tokens = []\n",
        "    line_number = 1\n",
        "    with open(source_code_file, 'r') as file:\n",
        "        for line in file:\n",
        "            for match in re.finditer(pattern, line):\n",
        "                for group_id, group in enumerate(match.groups()):\n",
        "                    if group:\n",
        "                        token_type = list(TokenType.__dict__.values())[group_id]\n",
        "                        lexeme = group\n",
        "                        if token_type == TokenType.COMMENT:\n",
        "                            break\n",
        "                        if token_type == TokenType.IDENTIFIER and len(lexeme) > 20:\n",
        "                            print(f\"Lexical Error: Identifier too long at line {line_number}\")\n",
        "                            return\n",
        "                        tokens.append((token_type, lexeme, line_number))\n",
        "                        break\n",
        "            line_number += 1\n",
        "    return tokens\n",
        "\n",
        "# Example usage\n",
        "source_code_file = \"input.code\"\n",
        "tokens = tokenize(source_code_file)\n",
        "for token_type, lexeme, line_number in tokens:\n",
        "    print(f\"Token: {token_type}, Lexeme: {lexeme}, Line: {line_number}\")\n"
      ]
    }
  ]
}